## Local LLM
ENDPOINT_URL=http://locahost:9292/v1/chat/completions
API_KEY=fakey
# You need to install them via llama-server or ollama
MODEL_NAMES=gemma-3-12b-it-Q4_K_M,Qwen3-4B-IQ4_NL,Qwen3-Coder-30B-A3B-Instruct,gpt-oss-20b-mxfp4
MODEL_EVALUATOR=gpt-oss-20b-mxfp4
THROTTLING_SECS=0.1

# or ------ OPENROUTER EXAMPLE
# Different providers could have different models name
# ENDPOINT_URL=https://openrouter.ai/api/v1/chat/completions
# API_KEY=sk-or-v1-your-api-key
# MODEL_NAMES=qwen/qwen3-4b:free,qwen/qwen3-30b-a3b:free,google/gemma-3-12b-it:free,openai/gpt-oss-20b:free,qwen/qwen3-coder:free
# MODEL_EVALUATOR=openai/gpt-oss-20b:free
# # THROTTLING_SECS is important or you get: Error for xyz.txt with openai/gpt-oss-20b:free: 429 Client Error: Too Many Requests for url: https://openrouter.ai/api/v1/chat/completions
# THROTTLING_SECS=37.0
